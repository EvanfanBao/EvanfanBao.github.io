 
<!DOCTYPE html>
<head>
    <meta charset="utf-8"/>
    <title>Mosaicking to Distill: Knowledge Distillation from Out-of-Domain </title>
    <link rel="shortcut icon" href="images/nips2021.png">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css"
          integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">
    <meta content="width=device-width, initial-scale=1" name="viewport"/>
    <link href="min.css" rel="stylesheet" type="text/css"/>
    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
    <script type="text/javascript">
        WebFont.load({google: {families: ["Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic", "Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic", "Ubuntu:300,300italic,400,400italic,500,500italic,700,700italic", "Changa One:400,400italic", "Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic", "Varela Round:400", "Bungee Shade:regular", "Roboto:300,regular,500"]}});</script>
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"
            type="text/javascript"></script><![endif]-->
    <script type="text/javascript">!function (o, c) {
        var n = c.documentElement, t = " w-mod-";
        n.className += t + "js", ("ontouchstart" in o || o.DocumentTouch && c instanceof DocumentTouch) && (n.className += t + "touch")
    }(window, document);</script>
    <link href="images/thumbnail.jpg"
          rel="shortcut icon" type="image/x-icon"/>
    <style>
        .wf-loading * {
            opacity: 0;
        }
    </style>
</head>
<body>


<div class="row">
    <div class="cell"><img src="images/nips2021.png" width="200"></div>
</div>

<div class="section hero p2m">
    <div class="container-2 p2m_header_v2 w-container"><h1 class="title">
        Mosaicking to Distill: Knowledge Distillation from Out-of-Domain Data</h1>
        <table align="center" border="0" width="850" class="authors">
            <tbody><tr>
            <td class="author"> <a href="https://www.vipazoo.cn/people/fanggongfan.html">Gongfan Fang</a><sup>1</sup></td>
            <td class="author"> <a href="https://evanfanbao.github.io">Yifan Bao</a><sup>1</sup></td>
            <td class="author"> <a href="https://vipazoo.cn/people/songjie">Jie Song</a><sup>1</sup></td>
            <td class="author"> <a href="https://sites.google.com/site/sitexinchaowang/">Xinchao Wang</a><sup>2</sup></td>
            <td class="author"> <a href="https://www.vipazoo.cn/people/xiedonglin">Donglin Xie</a><sup>1</sup></td>
            <td class="author"> <a href="https://chengchaoshen.github.io">Chengchao Shen</a><sup>1</sup></td>
            <td class="author"> <a href="https://person.zju.edu.cn/en/msong">Mingli Song</a><sup>1</sup></td>
            </tr></tbody>
        </table>
        <!-- <div class="p2m_authors_list_single w-row">
            <div class="w-col w-col-3 w-col-small-2 w-col-tiny-6">
                <a class="authors" href="https://www.vipazoo.cn/people/fanggongfan.html">Gongfan Fang
                </a></div>
            <div class="w-col w-col-3 w-col-small-2 w-col-tiny-6">
                <a class="authors" href="https://evanfanbao.github.io" target="_blank">Yifan Bao
                
                </a></div>
            <div class="w-col w-col-3 w-col-small-2 w-col-tiny-6">
                <a class="authors" href="https://vipazoo.cn/people/songjie" target="_blank">Jie Song</a></div>
            <div class="w-col w-col-3 w-col-small-2 w-col-tiny-6">
                <a class="authors" href="https://sites.google.com/site/sitexinchaowang/" target="_blank">Xinchao Wang</a>
            </div>
            <br>
            <br>

            <div class="w-col w-col-3 w-col-small-2 w-col-tiny-6">
                <a class="authors" href="https://sites.google.com/site/sitexinchaowang/" target="_blank">Donglin Xie</a>
            </div>
            <div class="w-col w-col-3 w-col-small-2 w-col-tiny-6">
                <a class="authors" href="https://sites.google.com/site/sitexinchaowang/" target="_blank">Chenchao Shen</a>
            </div>
            <div class="w-col w-col-3 w-col-small-2 w-col-tiny-6">
                <a class="authors" href="https://sites.google.com/site/sitexinchaowang/" target="_blank">Mingli Song</a>
            </div>
        </div>    -->
        
            

            <!-- Gongfan Fang, Yifan Bao, Jie Song, Xinchao Wang, Donglin Xie, Chengchao Shen, Mingli Song -->



            <!-- Donglin Xie, Chengchao Shen, Mingli Song -->

        
        <div class="div-block-10">
            <div class="equal_v2"><a href="https://www.zju.edu.cn/english/">Zhejiang University</a><sup>1</sup></div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            <div class="equal_v2"><a href="https://www.nus.edu.sg">National University of Singapore</a><sup>2</sup></div>
            
            </br>
            <!-- <div class="equal_v3">*Joint First Authors</div> -->
        </div>

        <div>
            <br>
            <!-- <span class="center"><img src="images/dgts.gif"></span> -->
            <!--            <span class="center"><img src="https://gist.github.com/ranahanocka/9513a4444d38ea419a3d6922f3563831/raw/4d0b90de8beff343de236e198726e24eeda15b68/lizard.gif" alt="lizard.gif"></span>-->
        </div>

        <!--start links -->
        <div class="p2m_authors_list_single w-row">
            <div class="w-col w-col-4 w-col-small-3 w-col-tiny-4">
                <a class="authors" href="" target="_blank">
                    <a href="https://arxiv.org/pdf/1912.11006.pdf" target="_blank"><i
                            class="far fa-4x fa-file text-primary mb-3 "></i></a>
                </a></div>

            <div class="w-col w-col-4 w-col-small-3 w-col-tiny-4">
                <a class="authors" href="" target="_blank">
                    <a href="https://github.com/zju-vipa/MosaicKD" target="_blank"><i
                            class="fab fa-4x fa-github text-primary mb-3 "></i></a>
                </a></div>

            <div class="w-col w-col-4 w-col-small-3 w-col-tiny-4">
                <a class="authors" href="" target="_blank">
                    <a href="" target="_blank"><i
                            class="fas fa-thumbtack fa-4x text-primary mb-3 "></i></a>
                </a></div>
        </div>


        <div class="div-block-4 w-row">
            <div class="w-col w-col-4 w-col-small-3 w-col-tiny-4">
                <div class="text-block-2"><strong style="color:#18446c" class="icon-bold-text">Paper</strong></div>
            </div>
            <div class="w-col w-col-4 w-col-small-3 w-col-tiny-4">
                <div class="text-block-2">
                    <strong style="color:#18446c" class="icon-bold-text">Code</strong>
                </div>
            </div>
            <div class="w-col w-col-4 w-col-small-3 w-col-tiny-4">
                <div class="text-block-2"><strong style="color:#18446c" class="icon-bold-text">Slides</strong></div>
            </div>
        </div>
    </div>

    <!--  end links  -->
</div>
</div>

<div class="white_section">
    <div class="w-container"><h3 class="grey-heading">Abstract</h3>
        <p class="paragraph-3 the_text">
            Knowledge distillation (KD) aims to craft a compact student model that imitates the behavior of a pre-trained teacher
            in a target domain. Prior KD approaches, despite their gratifying results, have largely relied on the premise that <i>in-domain</i>
            data is  available to carry out the knowledge transfer. Such an assumption, unfortunately, in many cases violates the 
            pratical setting, since the original training data or even the data domain is often unreachable due to privacy or copyright
            reasons. In this paper, we attempt to tackle an ambitious task, termed as <i>out-of-domain</i> knowledge distillation (OOD-KD), which
            allows us to conduct KD using only OOD data that can be readily obtained at a very low cost. Admittedly, OOD-KD is by nature a highly 
            challenging task due to the agnostic domain gap. To this end, we introduce a handy yet surprisingly efficacious approach, dubbed as <i>MosaicKD</i>.
            The key insight behind MosaicKD lies in that, samples from various domains share common local patterns, even though their global semantic may 
            vary significantly; these shared local patterns, in turn, can be re-assembled analogous to mosaic tiling, to approximate the in-domain data 
            and to further alleviating the domain discrepancy. In MosaicKD, this is achieved through a four-player min-max game, in which a generator, a discriminator, a student network, are collectively trained
            in an adversarial manner, partially under the guidance of a pre-trained teacher. We validate MosaicKD over classification and semantic segmentation tasks across
            various benchmarks, and demonstrate that it yields results much superior to the state-of-the-art counterparts on OOD data.
            Our code can be found in the supplementary material and will be made publicly available. 
        </p>
    </div>
</div>

<div class="white_section">
    <div class="w-container"><h3 class="grey-heading">Overview</h3>
        <p class="paragraph-3 the_text">      
            The core idea of MosaicKD is to synthesize in-domain data, of which the local patterns imitate those from real-world OOD data,
            while the global distribution, assembled from local ones, is expected to fool the pre-trained teacher.  
            <span class="center"><img src="images/overview.png"></span>
            The framework of MosaicKD. We leverage the local patterns of OOD data and the category knowledge of teacher to synthesize locally-authentic and globally-legitimate samples for KD.
        </p>
    </div>
</div>

<div class="white_section">
    <div class="w-container"><h3 class="grey-heading">Full Algorithm</h3>
        <p class="paragraph-3 the_text">
            <span class="center"><img src="images/algorithm.png"></span>
            The generator, takes as input a random noise vector and learns to mosaic synthetic in-domain samples
            with locally-authentic and globally-legitimate distributions, under the supervision back-propagated from the other 
            three players. The discriminator, on the other hand, learns to distinguish local patches extracted from the real-world OOD 
            data and from the synthetic samples. The entire synthetic images are fed to both the pre-trained teacher and the to-be-trained
            student, based on which the teacher provides category knowledge for data synthesis and the student mimics the behavior of the 
            teacher so as to carry out KD.  
        </p>
    </div>
</div>

<div class="white_section">
    <div class="w-container"><h3 class="grey-heading">Results</h3>
        <p class="paragraph-3 the_text">
            Table 1 reports the results with teachers pre-trained on CIFAR-100. Here we use CIFAR-10, ImageNet, Places365
            and SVHN as OOD data to evaluate MosaicKD for OOD-KD.
			<span class="center"><img src="images/table1.png"></span>
            
            <h2 class="grey-heading">Fine-grained Classification & Results on Segmentation</h2>
            <span class="center"><img src="images/table23.png" width=900></span>
            <p style="padding-left:2em">As shown in Table 3, we conduct KD on Fine-grained datasets, using some general data as OOD data.</p>
            
            <p style="padding-left:2em">Semantic segmentation can also be viewed as a classification task, where the network is trained to predict the category
            of each pixel. We apply our method to the NYUv2 dataset. Results are shown in Table 4.</p>
        </p>
    </div>
</div>

<div class="white_section">
    <div class="w-container"><h3 class="grey-heading">Quantitative Analysis</h3>
        <p class="paragraph-3 the_text">
            <span class="center"><img src="images/figure3.png"></span>
            Figure 3 provides some statistical information of OOD data and generated samples, including the category balance
            predicted by teachers and the per-class FID scores. 
        </p>
        <p class="paragraph-3 the_text">
            <span class="center"><img src="images/table4.png"></span>
            As shown in Table 4, we evaluate our method with different patch sizes and report the test accuracy and patch FID. 
        </p>
    </div>
</div>

<div class="white_section">
    <div class="w-container"><h3 class="grey-heading">Ablation Study</h3>
        <p class="paragraph-3 the_text">
            <span class="center"><img src="images/figure4.png"></span>
            Figure 4 visualizes the synthetic data with and without patch learning, where the generator without patch learning
            is trapped by the semantic of OOD data, failing to synthesize in-domain categories like trees and apples. 
            <span class="center"><img src="images/tabfig5.png"></span>
            Table 5 reports the following settings: (a) MosaicKD (b) MosaicKD without patch learning (c) MosaicKD without discrimination (d) MosaicKD without adversarial training.
        </p>
    </div>
</div>




</body></html>