<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>
    Mosaicking to Distill: Knowledge Distillation from Out-of-Domain Data
    </title>
    <!-- Bootstrap -->
    <link href="css/bootstrap-4.4.1.css" rel="stylesheet">

    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css"
          integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">

  </head>
  <body>
    

    <section>
      <div class="jumbotron text-center mt-4">
        <div class="container">
          <div class="row">
            <div class="col-12">
              <h1>Mosaicking to Distill: Knowledge Distillation <br> from Out-of-Domain Data</h1>
        <img class="img-fluid" src="images/nips2021-new.png" width="30%" alt="">
			  <!-- <h4 style="color:#5a6268;"> NeurIPS 2021 (poster) </h4> -->
				<hr>
              <h5> <a href="https://www.vipazoo.cn/people/fanggongfan.html" target="_blank">Gongfan Fang<sup>1,4</sup></a>, <a href="https://evanfanbao.github.io" target="_blank">Yifan Bao<sup>1</sup></a>, <a href="https://vipazoo.cn/people/songjie" target="_blank">Jie Song<sup>1</sup></a>, <a href="https://sites.google.com/site/sitexinchaowang/" target="_blank">Xinchao Wang<sup>2</sup></a>,  <a href="https://www.vipazoo.cn/people/xiedonglin" target="_blank">Donglin Xie<sup>1</sup></a>, <a href="https://chengchaoshen.github.io/" target="_blank">Chengchao Shen<sup>3</sup></a>, <a href="https://person.zju.edu.cn/en/msong" target="_blank">Mingli Song<sup>1*</sup></a> </h5>
				<p><a href="https://www.zju.edu.cn/english/" target="_blank"><sup>1</sup>Zhejiang University</a>, <a href="https://www.nus.edu.sg/" target="_blank"><sup>2</sup>National University of Singapore</a>, <a href="https://en.csu.edu.cn" target="_blank"><sup>3</sup>Central South University</a></p>
        <p><a href="https://azft.alibaba.com" target="_blank"><sup>4</sup>Alibaba-Zhejiang University Joint Institute of Frontier Technologies</a></p>
             
              <p><a class="btn btn-secondary btn-lg" href="https://arxiv.org/abs/2110.15094" role="button" target="_blank">Paper</a> <a class="btn btn-secondary btn-lg" href="https://github.com/zju-vipa/MosaicKD" target="_blank" role="button">Code</a> <a class="btn btn-secondary btn-lg" href="https://www.dropbox.com/sh/w8xehuk7debnka3/AABhoazFReE_5mMeyvb4iUWoa?dl=0" target="_blank" role="button">Model</a> </p>
              
              
            </div>
          </div>
        </div>
      </div>
    </section>
    <section>
      <div class="container">
        <div class="row">
          <div class="col-12 text-center">
            <h2>Abstract</h2>
			  <hr style="margin-top:0px">
<!--			  <div><img class="img-fluid" src="images/teaser.jpg" alt=""></div>-->
			  <!-- <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="images/teaser_3.mp4" type="video/mp4">
                </video> -->
              <div><img class="img-fluid" src="images/vis.jpeg" alt="" width="70%"></img></div>
				<br><br>
            <p class="text-left">Knowledge distillation (KD) aims to craft a compact student model 
              that imitates the behavior of a pre-trained teacher in a target domain. 
              Prior KD approaches, despite their gratifying results, have largely relied on the premise 
              that in-domain data is available to carry out the knowledge transfer. Such an assumption, 
              unfortunately, in many cases violates the practical setting, since the original training data or even 
              the data domain is often unreachable due to privacy or copyright reasons. 
              In this paper, we attempt to tackle an ambitious task, termed as out-of-domain 
              knowledge distillation (OOD-KD), which allows us to conduct KD using only OOD data that can be readily obtained at a very low cost. 
              Admittedly, OOD- KD is by nature a highly challenging task due to the agnostic domain gap. 
              To this end, we introduce a handy yet surprisingly efficacious approach, dubbed as MosaicKD. 
              The key insight behind MosaicKD lies in that, samples from various domains share common local patterns, even though their global semantic may vary significantly; these shared local patterns, in turn, 
              can be re-assembled analogous to mosaic tiling, to approximate the in-domain data and to further alleviating the domain discrepancy. In MosaicKD, this is achieved through a four-player min-max game, in which a generator, a discriminator, a student network, are collectively trained in an adversarial manner, partially under the guidance of a pre-trained teacher. We validate MosaicKD over classification and semantic segmentation tasks across various benchmarks, and demonstrate that it yields results much superior to the state-of-the-art counterparts on OOD data.</p>
          </div>
        </div>
      </div>
    </section>
	 <br>
    <section>
      <div class="container">
        <div class="row">
          <div class="col-12 text-center">
            <h2>Method</h2>
			   <hr style="margin-top:0px">
            <p class="text-left">
            MosaicKD establishes a four-player minimax game between a generator G, a patch discriminator D, a teacher model T and a student model S. The generator, as those in prior GANs, takes as input a random noise vector and learns to mosaic synthetic in-domain samples with locally-authentic and globally-legitimate distributions, under the supervisions back-propagated from the other three players.

            </p>
          </div>
        </div>
        <div class="row" style="margin-top:5px">
          <div class="col-12 text-center">
            <img class="img-fluid" src="images/overview.png" width="70%" alt="">
          </div>
        </div>
      </div>
    </section>
	  <br>
	      <section>
      <div class="container">
        <div class="row">
          <div class="col-12 text-center">
            <h2>Full Algorithm</h2>
			  <hr style="margin-top:0px">
          
          </div>
        </div>
		 <div class="row">
          <div class="col-12 text-center">
			    
              <img class="img-fluid" src="images/algorithm.png" width="70%" alt="">
          </div>
        </div>
		  <br><br>
		<div class="row">
          <div class="col-12 text-center">
			  <h2>Results</h2>
			  <hr style="margin-top:0px">
            <p class="text-left"> Table 1 reports the results with teachers pre-trained on CIFAR-100. Here we use CIFAR-10, ImageNet, Places365 and SVHN as OOD data to evaluate MosaicKD for OOD-KD.
            </p>
          </div>
        </div>
		<div class="row">
          <div class="col-12 text-center">
			     <!-- <video width="70%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="images/fountain.mp4" type="video/mp4">
                </video> -->
                <img class="img-fluid" src="images/table1.png" width="70%" alt="">
          </div>
        </div>
      </div>
    </section>
	  	 <br><br>
    <section>
      <div class="container">
        <div class="row">
          <div class="col-12 text-center">
          <h2>Quantitative Analysis</h2>
			   <hr style="margin-top:0px">
            
          </div>
        </div>
		<div class="row">
          <div class="col-12 text-center">
			     <!-- <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="images/swithch2.mp4" type="video/mp4">
                </video> -->
              <img class="img-fluid" src="images/figure3.png" width="70%" alt="">
              <p class="text-left">
                Figure 3 provides some statistical information of OOD data and generated samples, including the category balance predicted by teachers and the per-class FID scores.
              </p>
              <img class="img-fluid" src="images/table4.png" width="70%" alt="">
              <p class="text-left">
                As shown in Table 4, we evaluate our method with different patch sizes and report the test accuracy and patch FID.
              </p>
          </div>
        </div>
      </div>
    </section>

    <br><br>
    <section>
      <div class="container">
        <div class="row">
          <div class="col-12 text-center">
          <h2>Acknowledgements</h2>
			   <hr style="margin-top:0px">
            
          </div>
        </div>
		<div class="row">
          <div class="col-12 text-center">
			     <!-- <video width="100%" playsinline="" autoplay="" loop="" preload="" muted="">
                    <source src="images/swithch2.mp4" type="video/mp4">
                </video> -->
              <p>
                This work is supported by National Natural Science Foundation of China (U20B2066, 61976186), Key Research and Development Program of Zhejiang Province (2020C01023), the Major Scientific Research Project of Zhejiang Lab (No. 2019KD0AC01), the Fundamental Research Funds for the Central Universities, Alibaba-Zhejiang University Joint Research Institute of Frontier Technologies, Start-Up Grant from National University of Singapore (R-263-000-E95-133), and MOE AcRF TIER 1 FRC Research Grant (R-263-000-F14-114).
              </p>
              <p>
                This website is adapted from <a href="https://lioryariv.github.io/idr/">Multiview Neural Surface Reconstruction 
                by Disentangling Geometry and Appearance</a> 
              </p>
          </div>
        </div>
      </div>
    </section>


	  <br>
    <footer class="text-center">
	  <div class="container">
        <div class="row ">
          <div class="col-12">
			<h2>Citation</h2>
			  <hr style="margin-top:0px">
			  <div class="bibtexsection">
        @article{fang2021mosaicking,
          title={Mosaicking to Distill: Knowledge Distillation from Out-of-Domain Data},
          author={Gongfan Fang and Yifan Bao and Jie Song
                  and Xinchao Wang and Donglin Xie and Chengchao Shen and Mingli Song},
          journal={arXiv preprint arXiv:2110.15094},
          year={2021}
        }
			  </div>
			  <hr>
		</div>
        </div>
      </div>
    </footer>
    <!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
    <script src="js/jquery-3.4.1.min.js"></script>
    <!-- Include all compiled plugins (below), or include individual files as needed -->
    <script src="js/popper.min.js"></script>
    <script src="js/bootstrap-4.4.1.js"></script>
  </body>
</html>